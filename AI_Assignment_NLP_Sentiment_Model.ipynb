{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUBt7uRi3cva"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the IMDb reviews dataset\n",
        "# This will download the data the first time you run it\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=('train', 'test'),\n",
        "    with_info=True,\n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "E6PckxZD4CZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what the data looks like\n",
        "for review, label in train_data.take(2):\n",
        "  print(\"Review:\", review.numpy().decode('utf-8'))\n",
        "  print(\"Label:\", label.numpy()) # 0 is negative, 1 is positive\n",
        "  print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "JndFOqcQ4Z7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# The 'as_dataframe' function failed. We will do it manually.\n",
        "# We will loop through the dataset and pull out every review and label\n",
        "# one by one and put them into Python lists.\n",
        "# This will take a moment.\n",
        "\n",
        "print(\"Manually extracting training data...\")\n",
        "train_reviews = []\n",
        "train_labels = []\n",
        "\n",
        "# This loop goes through the 'train_data' pipe, grabs each (review, label) pair\n",
        "for review, label in train_data:\n",
        "    # .numpy() converts from a TensorFlow 'Tensor' object to a numpy value\n",
        "    # .decode('utf-8') converts from bytes (b'...') to a normal string\n",
        "    train_reviews.append(review.numpy().decode('utf-8'))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "print(\"Manually extracting testing data...\")\n",
        "test_reviews = []\n",
        "test_labels = []\n",
        "\n",
        "# Do the same for the test data\n",
        "for review, label in test_data:\n",
        "    test_reviews.append(review.numpy().decode('utf-8'))\n",
        "    test_labels.append(label.numpy())\n",
        "\n",
        "# Now, create the DataFrame (the \"workbench table\") from our lists.\n",
        "# This is a reliable, standard way to build a DataFrame.\n",
        "train_df = pd.DataFrame({\n",
        "    'text': train_reviews,    # Column 1 is our list of review strings\n",
        "    'label': train_labels     # Column 2 is our list of 0s and 1s\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'text': test_reviews,\n",
        "    'label': test_labels\n",
        "})\n",
        "\n",
        "# Now, prove it worked. This is your sanity check.\n",
        "print(\"\\nTraining DataFrame Head (Manual Build):\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "h58ZWtUX7uG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the spaCy library and its small English model\n",
        "# The '-q' makes the output less noisy.\n",
        "!pip install -U spacy -q\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "mjsWydcU8MQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the small English model.\n",
        "# We disable the 'parser' and 'ner' components because we don't need them.\n",
        "# We ONLY need the 'tagger' (for parts of speech) and 'lemmatizer'.\n",
        "# This makes it MUCH faster.\n",
        "print(\"Loading spaCy model...\")\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1. Remove HTML tags using a simple regex\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Create a spaCy 'doc' object.\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # 4. Create a list of clean tokens (words)\n",
        "    clean_tokens = []\n",
        "    for token in doc:\n",
        "        # We check two things:\n",
        "        # 1. Is it NOT a stop word (like 'the', 'a', 'is')?\n",
        "        # 2. Is it alphabetic (NOT punctuation, NOT a number)?\n",
        "        if not token.is_stop and token.is_alpha:\n",
        "            # If it passes, we get its 'lemma_' (the root form)\n",
        "            clean_tokens.append(token.lemma_)\n",
        "\n",
        "    # 5. Join the clean tokens back into a single string\n",
        "    return ' '.join(clean_tokens)"
      ],
      "metadata": {
        "id": "AZvoWeSf8twt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: This will take several minutes. Be patient.\n",
        "print(\"Cleaning training data (this will take a while)...\")\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"Cleaning testing data (this will also take a while)...\")\n",
        "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"Cleaning complete.\")\n",
        "\n",
        "# Prove it worked. Show me the first review, raw vs. clean.\n",
        "print(\"\\n--- ORIGINAL REVIEW (Index 0) ---\")\n",
        "print(train_df['text'].iloc[0])\n",
        "print(\"\\n--- CLEANED REVIEW (Index 0) ---\")\n",
        "print(train_df['clean_text'].iloc[0])"
      ],
      "metadata": {
        "id": "1hEws8UG9NSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Initialize the vectorizer.\n",
        "# We'll limit it to the top 10,000 most frequent words.\n",
        "# If we used every single word, your model would be pointlessly huge.\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "print(\"Vectorizing training data...\")\n",
        "# 2. FIT and TRANSFORM on the training data.\n",
        "# This learns the vocabulary AND converts the text to numbers.\n",
        "X_train_tfidf = vectorizer.fit_transform(train_df['clean_text'])\n",
        "\n",
        "print(\"Vectorizing testing data...\")\n",
        "# 3. ONLY TRANSFORM on the testing data.\n",
        "# We use the *same vocabulary* learned from the training data.\n",
        "# This prevents 'data leakage' and is a critical step.\n",
        "X_test_tfidf = vectorizer.transform(test_df['clean_text'])\n",
        "\n",
        "# 4. Get the labels. These are already 0s and 1s, so they are ready.\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "# 5. Prove it worked. Show me the shape of your new data.\n",
        "print(\"\\n--- SHAPE OF TF-IDF MATRICES ---\")\n",
        "print(\"Train data shape:\", X_train_tfidf.shape)\n",
        "print(\"Test data shape:\", X_test_tfidf.shape)\n",
        "print(\"\\n--- SHAPE OF LABELS ---\")\n",
        "print(\"Train labels shape:\", y_train.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "mLAR396iMe0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "# --- STEP 4: Build and Train the Model ---\n",
        "\n",
        "# 1. Initialize the model. We'll use the defaults.\n",
        "# It's a workhorse. It doesn't need much tuning.\n",
        "model = LinearSVC(dual=True) # dual=True is often faster for n_samples > n_features, but good to set. We'll stick with True for robustness.\n",
        "\n",
        "print(\"Training the LinearSVC model...\")\n",
        "\n",
        "# 2. Train the model.\n",
        "start_time = time.time()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"--- Training finished in {end_time - start_time:.2f} seconds ---\")\n",
        "\n",
        "\n",
        "# --- STEP 5: Evaluate the Model (The RIGHT Way) ---\n",
        "\n",
        "print(\"\\nMaking predictions on the test data...\")\n",
        "# 1. Make predictions on the unseen test data.\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# 2. Generate the Confusion Matrix.\n",
        "# This shows you exactly where you were wrong.\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(\"   (Predicted Neg) (Predicted Pos)\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 3. Generate the Classification Report.\n",
        "# This is your main result. It shows precision, recall, and f1-score.\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative (0)', 'Positive (1)']))"
      ],
      "metadata": {
        "id": "vlVgZbwXNkwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- NER Demonstration on Sample Reviews ---\")\n",
        "\n",
        "# We're not using the 'clean_text' function. NER needs real, raw text.\n",
        "# We must load a model that *has* an NER component.\n",
        "# nlp = spacy.load('en_core_web_sm') # If you disabled NER, load a fresh one\n",
        "\n",
        "sample_reviews = [\n",
        "    \"I bought a new Sony PlayStation 5 from Amazon, and it's amazing.\",\n",
        "    \"My old Samsung TV is better than this new Vizio one.\",\n",
        "    \"The new album by U2 is not as good as their work in the 90s.\"\n",
        "]\n",
        "\n",
        "for review in sample_reviews:\n",
        "    print(f\"\\nREVIEW: '{review}'\")\n",
        "    doc = nlp(review)\n",
        "\n",
        "    if not doc.ents:\n",
        "        print(\"  -> No entities found.\")\n",
        "    else:\n",
        "        print(\"  -> ENTITIES FOUND:\")\n",
        "        for ent in doc.ents:\n",
        "            # ent.text is the word, ent.label_ is the category\n",
        "            print(f\"    - {ent.text} ({ent.label_})\")"
      ],
      "metadata": {
        "id": "-WwUKhl7kJ9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}